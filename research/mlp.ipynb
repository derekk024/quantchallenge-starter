{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baa2146",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "train = pd.read_csv('./data/train.csv')\n",
    "test = pd.read_csv('./data/test.csv')\n",
    "train_new = pd.read_csv('./data/train_new.csv')\n",
    "test_new = pd.read_csv('./data/test_new.csv')\n",
    "\n",
    "train = pd.concat([train, train_new], axis=1)\n",
    "test = pd.concat([test, test_new], axis=1)\n",
    "train = train.fillna(-999)\n",
    "test = test.fillna(-999)\n",
    "\n",
    "features = [c for c in train.columns if c not in [\"id\",\"Y1\", \"Y2\"]]\n",
    "\n",
    "X = train[features].values\n",
    "X_test = test[features].values\n",
    "y2 = train[\"Y2\"].values\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(train[features].values)\n",
    "X_test = scaler.transform(test[features].values)\n",
    "\n",
    "n_features = X.shape[1]\n",
    "\n",
    "mlp_model = Sequential([\n",
    "    Input(shape=(n_features,)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "\n",
    "mlp_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "hist = mlp_model.fit(X, y2, \n",
    "                     epochs=50, \n",
    "                     batch_size=64, \n",
    "                     callbacks=[early_stop], \n",
    "                     validation_split=0.2,\n",
    "                     verbose=1)\n",
    "y2_pred = mlp_model.predict(X_test).flatten()\n",
    "y1_base_pred = np.full(len(test), train[\"Y1\"].mean())\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": test[\"id\"],\n",
    "    \"Y1\": y1_base_pred,\n",
    "    \"Y2\": y2_pred\n",
    "})\n",
    "submission.to_csv(\"mlp_submission.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfd9977b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.3729 - val_loss: 0.2837 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3360 - val_loss: 0.2891 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3098 - val_loss: 0.3195 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2921 - val_loss: 0.2662 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2947 - val_loss: 0.2853 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2846 - val_loss: 0.2841 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2825 - val_loss: 0.2814 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2851 - val_loss: 0.2893 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2794 - val_loss: 0.2661 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2748 - val_loss: 0.2679 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2729 - val_loss: 0.2498 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2771 - val_loss: 0.2302 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2791 - val_loss: 0.2836 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2579 - val_loss: 0.2570 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2691 - val_loss: 0.2697 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2629 - val_loss: 0.2466 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m498/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2618\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2643 - val_loss: 0.2330 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2639 - val_loss: 0.2628 - learning_rate: 5.0000e-04\n",
      "Epoch 19/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2545 - val_loss: 0.2572 - learning_rate: 5.0000e-04\n",
      "Epoch 20/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2523 - val_loss: 0.2220 - learning_rate: 5.0000e-04\n",
      "Epoch 21/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2553 - val_loss: 0.2934 - learning_rate: 5.0000e-04\n",
      "Epoch 22/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2545 - val_loss: 0.2110 - learning_rate: 5.0000e-04\n",
      "Epoch 23/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2486 - val_loss: 0.2438 - learning_rate: 5.0000e-04\n",
      "Epoch 24/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2520 - val_loss: 0.2313 - learning_rate: 5.0000e-04\n",
      "Epoch 25/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2436 - val_loss: 0.2806 - learning_rate: 5.0000e-04\n",
      "Epoch 26/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2458 - val_loss: 0.2411 - learning_rate: 5.0000e-04\n",
      "Epoch 27/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2372\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2477 - val_loss: 0.2495 - learning_rate: 5.0000e-04\n",
      "Epoch 28/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2498 - val_loss: 0.2428 - learning_rate: 2.5000e-04\n",
      "Epoch 29/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2422 - val_loss: 0.2454 - learning_rate: 2.5000e-04\n",
      "Epoch 30/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2486 - val_loss: 0.2460 - learning_rate: 2.5000e-04\n",
      "Epoch 31/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2381 - val_loss: 0.2488 - learning_rate: 2.5000e-04\n",
      "Epoch 32/200\n",
      "\u001b[1m499/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2391\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2391 - val_loss: 0.2361 - learning_rate: 2.5000e-04\n",
      "Epoch 33/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2426 - val_loss: 0.2219 - learning_rate: 1.2500e-04\n",
      "Epoch 34/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2378 - val_loss: 0.2435 - learning_rate: 1.2500e-04\n",
      "Epoch 35/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2391 - val_loss: 0.2277 - learning_rate: 1.2500e-04\n",
      "Epoch 36/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2361 - val_loss: 0.2444 - learning_rate: 1.2500e-04\n",
      "Epoch 37/200\n",
      "\u001b[1m482/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2276\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2411 - val_loss: 0.2415 - learning_rate: 1.2500e-04\n",
      "Epoch 38/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.2279 - val_loss: 0.2424 - learning_rate: 6.2500e-05\n",
      "Epoch 39/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.2365 - val_loss: 0.2336 - learning_rate: 6.2500e-05\n",
      "Epoch 40/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2318 - val_loss: 0.2414 - learning_rate: 6.2500e-05\n",
      "Epoch 41/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2312 - val_loss: 0.2411 - learning_rate: 6.2500e-05\n",
      "Epoch 42/200\n",
      "\u001b[1m498/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2313\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2285 - val_loss: 0.2524 - learning_rate: 6.2500e-05\n",
      "Epoch 43/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2327 - val_loss: 0.2389 - learning_rate: 3.1250e-05\n",
      "Epoch 44/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2294 - val_loss: 0.2359 - learning_rate: 3.1250e-05\n",
      "Epoch 45/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2305 - val_loss: 0.2393 - learning_rate: 3.1250e-05\n",
      "Epoch 46/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2374 - val_loss: 0.2394 - learning_rate: 3.1250e-05\n",
      "Epoch 47/200\n",
      "\u001b[1m492/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2410\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2352 - val_loss: 0.2354 - learning_rate: 3.1250e-05\n",
      "Epoch 48/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2345 - val_loss: 0.2497 - learning_rate: 1.5625e-05\n",
      "Epoch 49/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2333 - val_loss: 0.2347 - learning_rate: 1.5625e-05\n",
      "Epoch 50/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2345 - val_loss: 0.2432 - learning_rate: 1.5625e-05\n",
      "Epoch 51/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2325 - val_loss: 0.2420 - learning_rate: 1.5625e-05\n",
      "Epoch 52/200\n",
      "\u001b[1m494/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2221\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2336 - val_loss: 0.2453 - learning_rate: 1.5625e-05\n",
      "Epoch 52: early stopping\n",
      "Restoring model weights from the end of the best epoch: 22.\n",
      "Fold 1 R2: 0.7631340494523909\n",
      "Epoch 1/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.3915 - val_loss: 0.3696 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.3448 - val_loss: 0.2552 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3263 - val_loss: 0.2424 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3179 - val_loss: 0.2351 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3116 - val_loss: 0.2560 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3091 - val_loss: 0.2229 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2992 - val_loss: 0.2656 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3042 - val_loss: 0.2125 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2940 - val_loss: 0.2307 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2871 - val_loss: 0.2248 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2975 - val_loss: 0.2209 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2919 - val_loss: 0.2225 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2998 - val_loss: 0.2083 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2793 - val_loss: 0.2372 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2779 - val_loss: 0.2137 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2860 - val_loss: 0.2207 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2823 - val_loss: 0.2151 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m489/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2584\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2758 - val_loss: 0.2089 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2728 - val_loss: 0.2133 - learning_rate: 5.0000e-04\n",
      "Epoch 20/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2734 - val_loss: 0.2183 - learning_rate: 5.0000e-04\n",
      "Epoch 21/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2652 - val_loss: 0.2029 - learning_rate: 5.0000e-04\n",
      "Epoch 22/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2522 - val_loss: 0.2027 - learning_rate: 5.0000e-04\n",
      "Epoch 23/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2673 - val_loss: 0.2061 - learning_rate: 5.0000e-04\n",
      "Epoch 24/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2626 - val_loss: 0.2079 - learning_rate: 5.0000e-04\n",
      "Epoch 25/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.2570 - val_loss: 0.2170 - learning_rate: 5.0000e-04\n",
      "Epoch 26/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2579 - val_loss: 0.2041 - learning_rate: 5.0000e-04\n",
      "Epoch 27/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2504 - val_loss: 0.1984 - learning_rate: 5.0000e-04\n",
      "Epoch 28/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2605 - val_loss: 0.2131 - learning_rate: 5.0000e-04\n",
      "Epoch 29/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2531 - val_loss: 0.2042 - learning_rate: 5.0000e-04\n",
      "Epoch 30/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2611 - val_loss: 0.2084 - learning_rate: 5.0000e-04\n",
      "Epoch 31/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2469 - val_loss: 0.1988 - learning_rate: 5.0000e-04\n",
      "Epoch 32/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2588 - val_loss: 0.1933 - learning_rate: 5.0000e-04\n",
      "Epoch 33/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2587 - val_loss: 0.2085 - learning_rate: 5.0000e-04\n",
      "Epoch 34/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2635 - val_loss: 0.2127 - learning_rate: 5.0000e-04\n",
      "Epoch 35/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2470 - val_loss: 0.2037 - learning_rate: 5.0000e-04\n",
      "Epoch 36/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2617 - val_loss: 0.2106 - learning_rate: 5.0000e-04\n",
      "Epoch 37/200\n",
      "\u001b[1m490/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2313\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2422 - val_loss: 0.2098 - learning_rate: 5.0000e-04\n",
      "Epoch 38/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2452 - val_loss: 0.2057 - learning_rate: 2.5000e-04\n",
      "Epoch 39/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2436 - val_loss: 0.2101 - learning_rate: 2.5000e-04\n",
      "Epoch 40/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2423 - val_loss: 0.1976 - learning_rate: 2.5000e-04\n",
      "Epoch 41/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2433 - val_loss: 0.2114 - learning_rate: 2.5000e-04\n",
      "Epoch 42/200\n",
      "\u001b[1m478/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2474\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2368 - val_loss: 0.1975 - learning_rate: 2.5000e-04\n",
      "Epoch 43/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2413 - val_loss: 0.1987 - learning_rate: 1.2500e-04\n",
      "Epoch 44/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2359 - val_loss: 0.1989 - learning_rate: 1.2500e-04\n",
      "Epoch 45/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2365 - val_loss: 0.2013 - learning_rate: 1.2500e-04\n",
      "Epoch 46/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2434 - val_loss: 0.2054 - learning_rate: 1.2500e-04\n",
      "Epoch 47/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2330\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2404 - val_loss: 0.2001 - learning_rate: 1.2500e-04\n",
      "Epoch 48/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2373 - val_loss: 0.2021 - learning_rate: 6.2500e-05\n",
      "Epoch 49/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2369 - val_loss: 0.2004 - learning_rate: 6.2500e-05\n",
      "Epoch 50/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2365 - val_loss: 0.2044 - learning_rate: 6.2500e-05\n",
      "Epoch 51/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2336 - val_loss: 0.2010 - learning_rate: 6.2500e-05\n",
      "Epoch 52/200\n",
      "\u001b[1m482/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2318\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2357 - val_loss: 0.2033 - learning_rate: 6.2500e-05\n",
      "Epoch 53/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2409 - val_loss: 0.2016 - learning_rate: 3.1250e-05\n",
      "Epoch 54/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2367 - val_loss: 0.2014 - learning_rate: 3.1250e-05\n",
      "Epoch 55/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2271 - val_loss: 0.2028 - learning_rate: 3.1250e-05\n",
      "Epoch 56/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2361 - val_loss: 0.1988 - learning_rate: 3.1250e-05\n",
      "Epoch 57/200\n",
      "\u001b[1m487/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2459\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2471 - val_loss: 0.1992 - learning_rate: 3.1250e-05\n",
      "Epoch 58/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2399 - val_loss: 0.2033 - learning_rate: 1.5625e-05\n",
      "Epoch 59/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2310 - val_loss: 0.2012 - learning_rate: 1.5625e-05\n",
      "Epoch 60/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2346 - val_loss: 0.2031 - learning_rate: 1.5625e-05\n",
      "Epoch 61/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2388 - val_loss: 0.1999 - learning_rate: 1.5625e-05\n",
      "Epoch 62/200\n",
      "\u001b[1m476/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2173\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2304 - val_loss: 0.2003 - learning_rate: 1.5625e-05\n",
      "Epoch 62: early stopping\n",
      "Restoring model weights from the end of the best epoch: 32.\n",
      "Fold 2 R2: 0.7438395128087321\n",
      "Epoch 1/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.3922 - val_loss: 0.2374 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.3403 - val_loss: 0.2501 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3265 - val_loss: 0.2477 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3206 - val_loss: 0.2481 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3130 - val_loss: 0.2424 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3055 - val_loss: 0.2269 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2949 - val_loss: 0.2410 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3036 - val_loss: 0.2313 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2877 - val_loss: 0.2228 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2861 - val_loss: 0.2090 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2956 - val_loss: 0.2147 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2882 - val_loss: 0.2040 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2917 - val_loss: 0.2238 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2890 - val_loss: 0.2015 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2840 - val_loss: 0.2117 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2813 - val_loss: 0.2201 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2777 - val_loss: 0.2009 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2739 - val_loss: 0.2099 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2855 - val_loss: 0.1887 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2758 - val_loss: 0.2065 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2842 - val_loss: 0.2181 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2752 - val_loss: 0.2098 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2597 - val_loss: 0.2124 - learning_rate: 0.0010\n",
      "Epoch 24/200\n",
      "\u001b[1m483/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2933\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2763 - val_loss: 0.2205 - learning_rate: 0.0010\n",
      "Epoch 25/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2552 - val_loss: 0.1971 - learning_rate: 5.0000e-04\n",
      "Epoch 26/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2510 - val_loss: 0.2052 - learning_rate: 5.0000e-04\n",
      "Epoch 27/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2591 - val_loss: 0.2022 - learning_rate: 5.0000e-04\n",
      "Epoch 28/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2516 - val_loss: 0.2071 - learning_rate: 5.0000e-04\n",
      "Epoch 29/200\n",
      "\u001b[1m494/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2694\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2443 - val_loss: 0.2102 - learning_rate: 5.0000e-04\n",
      "Epoch 30/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2566 - val_loss: 0.2020 - learning_rate: 2.5000e-04\n",
      "Epoch 31/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2557 - val_loss: 0.2091 - learning_rate: 2.5000e-04\n",
      "Epoch 32/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2461 - val_loss: 0.2090 - learning_rate: 2.5000e-04\n",
      "Epoch 33/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2445 - val_loss: 0.2160 - learning_rate: 2.5000e-04\n",
      "Epoch 34/200\n",
      "\u001b[1m498/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2444\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2457 - val_loss: 0.2186 - learning_rate: 2.5000e-04\n",
      "Epoch 35/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2394 - val_loss: 0.2067 - learning_rate: 1.2500e-04\n",
      "Epoch 36/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2360 - val_loss: 0.2103 - learning_rate: 1.2500e-04\n",
      "Epoch 37/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2373 - val_loss: 0.2022 - learning_rate: 1.2500e-04\n",
      "Epoch 38/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2378 - val_loss: 0.2012 - learning_rate: 1.2500e-04\n",
      "Epoch 39/200\n",
      "\u001b[1m498/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2439\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2544 - val_loss: 0.2037 - learning_rate: 1.2500e-04\n",
      "Epoch 40/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2423 - val_loss: 0.2028 - learning_rate: 6.2500e-05\n",
      "Epoch 41/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2361 - val_loss: 0.2075 - learning_rate: 6.2500e-05\n",
      "Epoch 42/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2366 - val_loss: 0.2021 - learning_rate: 6.2500e-05\n",
      "Epoch 43/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2450 - val_loss: 0.2096 - learning_rate: 6.2500e-05\n",
      "Epoch 44/200\n",
      "\u001b[1m481/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2484\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2436 - val_loss: 0.2041 - learning_rate: 6.2500e-05\n",
      "Epoch 45/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2403 - val_loss: 0.2029 - learning_rate: 3.1250e-05\n",
      "Epoch 46/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2487 - val_loss: 0.2054 - learning_rate: 3.1250e-05\n",
      "Epoch 47/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2364 - val_loss: 0.2023 - learning_rate: 3.1250e-05\n",
      "Epoch 48/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2508 - val_loss: 0.2044 - learning_rate: 3.1250e-05\n",
      "Epoch 49/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2463\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2372 - val_loss: 0.2086 - learning_rate: 3.1250e-05\n",
      "Epoch 49: early stopping\n",
      "Restoring model weights from the end of the best epoch: 19.\n",
      "Fold 3 R2: 0.756707459182173\n",
      "Epoch 1/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.3616 - val_loss: 0.3123 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.3136 - val_loss: 0.2737 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2947 - val_loss: 0.3006 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2965 - val_loss: 0.3301 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2818 - val_loss: 0.2649 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2877 - val_loss: 0.2610 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2854 - val_loss: 0.2630 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2793 - val_loss: 0.2805 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2692 - val_loss: 0.3227 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2705 - val_loss: 0.3030 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m487/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2695\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2743 - val_loss: 0.2803 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2604 - val_loss: 0.2794 - learning_rate: 5.0000e-04\n",
      "Epoch 13/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2600 - val_loss: 0.2605 - learning_rate: 5.0000e-04\n",
      "Epoch 14/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2566 - val_loss: 0.2596 - learning_rate: 5.0000e-04\n",
      "Epoch 15/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2594 - val_loss: 0.2969 - learning_rate: 5.0000e-04\n",
      "Epoch 16/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2487 - val_loss: 0.2428 - learning_rate: 5.0000e-04\n",
      "Epoch 17/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2478 - val_loss: 0.2717 - learning_rate: 5.0000e-04\n",
      "Epoch 18/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2548 - val_loss: 0.2506 - learning_rate: 5.0000e-04\n",
      "Epoch 19/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2437 - val_loss: 0.2726 - learning_rate: 5.0000e-04\n",
      "Epoch 20/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2501 - val_loss: 0.2773 - learning_rate: 5.0000e-04\n",
      "Epoch 21/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2481 - val_loss: 0.2346 - learning_rate: 5.0000e-04\n",
      "Epoch 22/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2544 - val_loss: 0.2733 - learning_rate: 5.0000e-04\n",
      "Epoch 23/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2450 - val_loss: 0.2438 - learning_rate: 5.0000e-04\n",
      "Epoch 24/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2412 - val_loss: 0.2846 - learning_rate: 5.0000e-04\n",
      "Epoch 25/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2463 - val_loss: 0.2453 - learning_rate: 5.0000e-04\n",
      "Epoch 26/200\n",
      "\u001b[1m489/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2503\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2426 - val_loss: 0.2753 - learning_rate: 5.0000e-04\n",
      "Epoch 27/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2417 - val_loss: 0.2442 - learning_rate: 2.5000e-04\n",
      "Epoch 28/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2402 - val_loss: 0.2606 - learning_rate: 2.5000e-04\n",
      "Epoch 29/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2366 - val_loss: 0.2587 - learning_rate: 2.5000e-04\n",
      "Epoch 30/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2327 - val_loss: 0.2607 - learning_rate: 2.5000e-04\n",
      "Epoch 31/200\n",
      "\u001b[1m479/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2332\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2326 - val_loss: 0.2456 - learning_rate: 2.5000e-04\n",
      "Epoch 32/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2366 - val_loss: 0.2522 - learning_rate: 1.2500e-04\n",
      "Epoch 33/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2353 - val_loss: 0.2540 - learning_rate: 1.2500e-04\n",
      "Epoch 34/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2353 - val_loss: 0.2568 - learning_rate: 1.2500e-04\n",
      "Epoch 35/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2363 - val_loss: 0.2468 - learning_rate: 1.2500e-04\n",
      "Epoch 36/200\n",
      "\u001b[1m484/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2233\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2380 - val_loss: 0.2457 - learning_rate: 1.2500e-04\n",
      "Epoch 37/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2258 - val_loss: 0.2523 - learning_rate: 6.2500e-05\n",
      "Epoch 38/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2311 - val_loss: 0.2560 - learning_rate: 6.2500e-05\n",
      "Epoch 39/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2277 - val_loss: 0.2520 - learning_rate: 6.2500e-05\n",
      "Epoch 40/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2301 - val_loss: 0.2512 - learning_rate: 6.2500e-05\n",
      "Epoch 41/200\n",
      "\u001b[1m488/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2307\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2315 - val_loss: 0.2432 - learning_rate: 6.2500e-05\n",
      "Epoch 42/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2229 - val_loss: 0.2505 - learning_rate: 3.1250e-05\n",
      "Epoch 43/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2259 - val_loss: 0.2403 - learning_rate: 3.1250e-05\n",
      "Epoch 44/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2308 - val_loss: 0.2470 - learning_rate: 3.1250e-05\n",
      "Epoch 45/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2304 - val_loss: 0.2527 - learning_rate: 3.1250e-05\n",
      "Epoch 46/200\n",
      "\u001b[1m479/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2368\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2260 - val_loss: 0.2464 - learning_rate: 3.1250e-05\n",
      "Epoch 47/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2382 - val_loss: 0.2504 - learning_rate: 1.5625e-05\n",
      "Epoch 48/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2224 - val_loss: 0.2472 - learning_rate: 1.5625e-05\n",
      "Epoch 49/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2345 - val_loss: 0.2471 - learning_rate: 1.5625e-05\n",
      "Epoch 50/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2293 - val_loss: 0.2491 - learning_rate: 1.5625e-05\n",
      "Epoch 51/200\n",
      "\u001b[1m494/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2178\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2177 - val_loss: 0.2493 - learning_rate: 1.5625e-05\n",
      "Epoch 51: early stopping\n",
      "Restoring model weights from the end of the best epoch: 21.\n",
      "Fold 4 R2: 0.7532561994498451\n",
      "Epoch 1/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.3826 - val_loss: 0.3047 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.3294 - val_loss: 0.2978 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3128 - val_loss: 0.3613 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2998 - val_loss: 0.3205 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2975 - val_loss: 0.2685 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2876 - val_loss: 0.2778 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2897 - val_loss: 0.2791 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2882 - val_loss: 0.2588 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2722 - val_loss: 0.2811 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2796 - val_loss: 0.3185 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2693 - val_loss: 0.2705 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2697 - val_loss: 0.2819 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m488/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2591\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2791 - val_loss: 0.2817 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2653 - val_loss: 0.2747 - learning_rate: 5.0000e-04\n",
      "Epoch 15/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2631 - val_loss: 0.2737 - learning_rate: 5.0000e-04\n",
      "Epoch 16/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.2552 - val_loss: 0.2642 - learning_rate: 5.0000e-04\n",
      "Epoch 17/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2562 - val_loss: 0.2722 - learning_rate: 5.0000e-04\n",
      "Epoch 18/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2428\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2598 - val_loss: 0.2626 - learning_rate: 5.0000e-04\n",
      "Epoch 19/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2566 - val_loss: 0.2693 - learning_rate: 2.5000e-04\n",
      "Epoch 20/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2453 - val_loss: 0.2625 - learning_rate: 2.5000e-04\n",
      "Epoch 21/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2429 - val_loss: 0.2605 - learning_rate: 2.5000e-04\n",
      "Epoch 22/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2456 - val_loss: 0.2617 - learning_rate: 2.5000e-04\n",
      "Epoch 23/200\n",
      "\u001b[1m496/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2219\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2335 - val_loss: 0.2682 - learning_rate: 2.5000e-04\n",
      "Epoch 24/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2488 - val_loss: 0.2607 - learning_rate: 1.2500e-04\n",
      "Epoch 25/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2473 - val_loss: 0.2595 - learning_rate: 1.2500e-04\n",
      "Epoch 26/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2341 - val_loss: 0.2603 - learning_rate: 1.2500e-04\n",
      "Epoch 27/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2430 - val_loss: 0.2598 - learning_rate: 1.2500e-04\n",
      "Epoch 28/200\n",
      "\u001b[1m495/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2446\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2429 - val_loss: 0.2611 - learning_rate: 1.2500e-04\n",
      "Epoch 29/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2402 - val_loss: 0.2640 - learning_rate: 6.2500e-05\n",
      "Epoch 30/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2286 - val_loss: 0.2660 - learning_rate: 6.2500e-05\n",
      "Epoch 31/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2339 - val_loss: 0.2678 - learning_rate: 6.2500e-05\n",
      "Epoch 32/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2245 - val_loss: 0.2635 - learning_rate: 6.2500e-05\n",
      "Epoch 33/200\n",
      "\u001b[1m478/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2326\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2344 - val_loss: 0.2603 - learning_rate: 6.2500e-05\n",
      "Epoch 34/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2271 - val_loss: 0.2603 - learning_rate: 3.1250e-05\n",
      "Epoch 35/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2361 - val_loss: 0.2619 - learning_rate: 3.1250e-05\n",
      "Epoch 36/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2274 - val_loss: 0.2574 - learning_rate: 3.1250e-05\n",
      "Epoch 37/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2299 - val_loss: 0.2603 - learning_rate: 3.1250e-05\n",
      "Epoch 38/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2336 - val_loss: 0.2577 - learning_rate: 3.1250e-05\n",
      "Epoch 39/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2315 - val_loss: 0.2613 - learning_rate: 3.1250e-05\n",
      "Epoch 40/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2323 - val_loss: 0.2613 - learning_rate: 3.1250e-05\n",
      "Epoch 41/200\n",
      "\u001b[1m492/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2322\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2306 - val_loss: 0.2583 - learning_rate: 3.1250e-05\n",
      "Epoch 42/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2313 - val_loss: 0.2574 - learning_rate: 1.5625e-05\n",
      "Epoch 43/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2366 - val_loss: 0.2628 - learning_rate: 1.5625e-05\n",
      "Epoch 44/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2362 - val_loss: 0.2648 - learning_rate: 1.5625e-05\n",
      "Epoch 45/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2327 - val_loss: 0.2609 - learning_rate: 1.5625e-05\n",
      "Epoch 46/200\n",
      "\u001b[1m478/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2435\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2323 - val_loss: 0.2611 - learning_rate: 1.5625e-05\n",
      "Epoch 47/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2290 - val_loss: 0.2583 - learning_rate: 7.8125e-06\n",
      "Epoch 48/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2349 - val_loss: 0.2602 - learning_rate: 7.8125e-06\n",
      "Epoch 49/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2305 - val_loss: 0.2650 - learning_rate: 7.8125e-06\n",
      "Epoch 50/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2376 - val_loss: 0.2615 - learning_rate: 7.8125e-06\n",
      "Epoch 51/200\n",
      "\u001b[1m480/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2281\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2302 - val_loss: 0.2615 - learning_rate: 7.8125e-06\n",
      "Epoch 52/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2315 - val_loss: 0.2628 - learning_rate: 3.9063e-06\n",
      "Epoch 53/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2295 - val_loss: 0.2603 - learning_rate: 3.9063e-06\n",
      "Epoch 54/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2338 - val_loss: 0.2598 - learning_rate: 3.9063e-06\n",
      "Epoch 55/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2445 - val_loss: 0.2645 - learning_rate: 3.9063e-06\n",
      "Epoch 56/200\n",
      "\u001b[1m489/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2219\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2307 - val_loss: 0.2620 - learning_rate: 3.9063e-06\n",
      "Epoch 57/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2305 - val_loss: 0.2613 - learning_rate: 1.9531e-06\n",
      "Epoch 58/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2365 - val_loss: 0.2626 - learning_rate: 1.9531e-06\n",
      "Epoch 59/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2308 - val_loss: 0.2607 - learning_rate: 1.9531e-06\n",
      "Epoch 60/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.2283 - val_loss: 0.2614 - learning_rate: 1.9531e-06\n",
      "Epoch 61/200\n",
      "\u001b[1m485/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2319\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2329 - val_loss: 0.2619 - learning_rate: 1.9531e-06\n",
      "Epoch 62/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2406 - val_loss: 0.2633 - learning_rate: 9.7656e-07\n",
      "Epoch 63/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2380 - val_loss: 0.2618 - learning_rate: 9.7656e-07\n",
      "Epoch 64/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2331 - val_loss: 0.2613 - learning_rate: 9.7656e-07\n",
      "Epoch 65/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2364 - val_loss: 0.2609 - learning_rate: 9.7656e-07\n",
      "Epoch 66/200\n",
      "\u001b[1m489/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2378\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2287 - val_loss: 0.2587 - learning_rate: 9.7656e-07\n",
      "Epoch 67/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2343 - val_loss: 0.2625 - learning_rate: 4.8828e-07\n",
      "Epoch 68/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2309 - val_loss: 0.2666 - learning_rate: 4.8828e-07\n",
      "Epoch 69/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2392 - val_loss: 0.2644 - learning_rate: 4.8828e-07\n",
      "Epoch 70/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2309 - val_loss: 0.2620 - learning_rate: 4.8828e-07\n",
      "Epoch 71/200\n",
      "\u001b[1m496/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2354\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2261 - val_loss: 0.2625 - learning_rate: 4.8828e-07\n",
      "Epoch 72/200\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2288 - val_loss: 0.2619 - learning_rate: 2.4414e-07\n",
      "Epoch 72: early stopping\n",
      "Restoring model weights from the end of the best epoch: 42.\n",
      "Fold 5 R2: 0.7120762066814936\n",
      "OOF R2: 0.7456611739697091\n",
      "Test R2 scores by fold: [0.7631340494523909, 0.7438395128087321, 0.756707459182173, 0.7532561994498451, 0.7120762066814936]\n",
      "Mean Fold R2: 0.7458026855149269 +/- 0.0179787349734128\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "\n",
    "train = pd.read_csv('./data/train.csv')\n",
    "test = pd.read_csv('./data/test.csv')\n",
    "train_new = pd.read_csv('./data/train_new.csv')\n",
    "test_new = pd.read_csv('./data/test_new.csv')\n",
    "\n",
    "train = pd.concat([train, train_new], axis=1)\n",
    "test = pd.concat([test, test_new], axis=1)\n",
    "\n",
    "train = train.fillna(-999)\n",
    "test = test.fillna(-999)\n",
    "\n",
    "features = [c for c in train.columns if c not in [\"id\",\"Y1\", \"Y2\"]]\n",
    "X_full = train[features].values\n",
    "X_test = test[features].values\n",
    "y2_full = train[\"Y2\"].values\n",
    "\n",
    "def make_model(inut_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(inut_dim,)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "oof = np.zeros(len(train))\n",
    "test_pred = np.zeros((len(test), kf.get_n_splits()))\n",
    "fold_scores = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_full),1):\n",
    "    X_tr, X_va = X_full[train_idx], X_full[val_idx]\n",
    "    y_tr, y_va = y2_full[train_idx], y2_full[val_idx]\n",
    "\n",
    "    x_scaler = StandardScaler().fit(X_tr)\n",
    "    X_tr_s = x_scaler.transform(X_tr)\n",
    "    X_va_s = x_scaler.transform(X_va)\n",
    "    X_tst_s = x_scaler.transform(X_test)\n",
    "\n",
    "    callbacks = [ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7, verbose=1),EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True, verbose=1)]\n",
    "    model = make_model(X_tr_s.shape[1])\n",
    "    history = model.fit(X_tr_s, y_tr, \n",
    "                        validation_data=(X_va_s, y_va),\n",
    "                        epochs=200, \n",
    "                        batch_size=128,\n",
    "                        callbacks=callbacks,\n",
    "                        verbose=1)\n",
    "    \n",
    "    va_pred = model.predict(X_va_s, verbose=0).ravel()\n",
    "    oof[val_idx] = va_pred\n",
    "\n",
    "    fold_r2 = r2_score(y_va, va_pred)\n",
    "    fold_scores.append(fold_r2)\n",
    "    print(f\"Fold {fold} R2: {fold_r2}\")\n",
    "\n",
    "    te_pred =  model.predict(X_tst_s, verbose=0).ravel()\n",
    "    test_pred[:, fold-1] = te_pred\n",
    "\n",
    "oof_r2 = r2_score(y2_full, oof)\n",
    "print(f\"OOF R2: {oof_r2}\")\n",
    "print(\"Test R2 scores by fold:\", fold_scores)\n",
    "print(f\"Mean Fold R2: {np.mean(fold_scores)} +/- {np.std(fold_scores)}\")\n",
    "\n",
    "y2_test_pred = test_pred.mean(axis=1)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": test[\"id\"],\n",
    "    \"Y1\": np.full(len(test), train[\"Y1\"].mean()),\n",
    "    \"Y2\": y2_test_pred\n",
    "})\n",
    "submission.to_csv(\"mlp_kfold_submission.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b0eb78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = \"final.csv\"\n",
    "y2file = \"mlp_kfold_submission.csv\"\n",
    "\n",
    "final_df = pd.read_csv(final)\n",
    "y2_df = pd.read_csv(y2file)\n",
    "\n",
    "y2_values = y2_df[['id',\"Y2\"]]\n",
    "updated_df = pd.merge(final_df.drop('Y2', axis=1), y2_values, on='id', how='left')\n",
    "updated_df.to_csv(\"final_updated.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
